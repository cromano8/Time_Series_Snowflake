{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.types import *\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# from utilities_time_series import establish_hourly_time_series, create_hourly_df, create_daily_df\n",
    "from utilities_time_series import generate_time_series_data, create_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.config(\"connection_name\", \"default\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='MOCKSERIES already exists, statement succeeded.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS MOCKSERIES\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db.schema: \"CROMANO\".\"MOCKSERIES\"\n"
     ]
    }
   ],
   "source": [
    "session.use_schema(\"MOCKSERIES\")\n",
    "print(f'db.schema: {session.get_fully_qualified_current_schema()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================\n",
    "# ESTABLISH WHAT PARTITIONS WILL BE USED (IF ANY)\n",
    "# ==================================================================================================\n",
    "# NOTE: CHANGE to specify zero, one, or multiple partition columns to match their use case\n",
    "\n",
    "# List of all partition column names followed by a list of the number of unique values for each corresponding partition column\n",
    "#partition_columns = ['STORE_ID', 'PRODUCT_ID'] \n",
    "partition_columns = ['STORE_ID']         \n",
    "unique_value_counts = [5000] # Multiply all the values in the list to get the total number or partitions in the data set                     \n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# ESTABLISH WHAT EXOGENOUS FEATURES YOU MIGHT WANT TO INCLUDE (IF ANY)\n",
    "# ==================================================================================================\n",
    "# NOTE: CHANGE to specify zero, one, or multiple exogenous features to match their use case\n",
    "\n",
    "exogenous_feature_columns = ['FEATURE_1', 'FEATURE_2']\n",
    "stddev_scale_factors = [2.5, 10.0] # LOWER numbers will make features that are MORE correlated with the TARGET                    \n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# SET START YEAR & GRAULARITY (daily, hourly, or weekly). \n",
    "# ==================================================================================================\n",
    "\n",
    "# Specify granularity. Example: 'hourly' will generate a time series value at each hour\n",
    "granularity = 'daily'\n",
    "\n",
    "# Specifying the start year will make the first data point occur on January 1st of that year\n",
    "start_year = 2021                \n",
    "\n",
    "\n",
    "# ==================================================================================================\n",
    "# PARAMETER RANGES FOR TIME SERIES FUNCTION\n",
    "# ==================================================================================================\n",
    "# NOTE: CHANGE these ranges to adjust qualities of the time series such as trend and seasonality\n",
    "\n",
    "# Ranges for random generation of parameter values  \n",
    "trend_coeff_range = (-0.75, 0.75)\n",
    "trend_flat_base_range = (50, 900)\n",
    "seasonality_daily_amp_range = (2, 15)\n",
    "seasonality_weekly_amp_range = (2, 25)\n",
    "seasonality_6mo_amp_range = (15, 40)\n",
    "seasonality_yearly_amp_range = (15, 50)\n",
    "noise_mean_range = (0, 0)\n",
    "noise_std_range = (1, 15)\n",
    "noise_corr_range = (0.25, 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate time series for each of many partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists, where each list is the unique values for the partition column at the corresponding index. \n",
    "value_lists = []\n",
    "for i in unique_value_counts:\n",
    "    value_lists.append(list(range(1,i+1)))\n",
    "\n",
    "# Create a list of all possible partition value combinations\n",
    "all_combos = list(itertools.product(*value_lists))\n",
    "number_of_partitions = len(all_combos)\n",
    "print(f\"Number of partitions: {number_of_partitions}\")\n",
    "\n",
    "# List to hold each partition's pandas DataFrame\n",
    "df_list = []\n",
    "\n",
    "# Loop through partitions and create time series for each partition\n",
    "for combo in all_combos:\n",
    "    time_points, ts_values = generate_time_series_data(\n",
    "        start_year = start_year,\n",
    "        grain = granularity,\n",
    "        trend_coeff = random.uniform(*trend_coeff_range),\n",
    "        trend_flat_base = random.randint(*trend_flat_base_range),\n",
    "        seasonality_daily_amp = random.randint(*seasonality_daily_amp_range),\n",
    "        seasonality_weekly_amp = random.randint(*seasonality_weekly_amp_range),\n",
    "        seasonality_6mo_amp = random.randint(*seasonality_6mo_amp_range),\n",
    "        seasonality_yearly_amp = random.randint(*seasonality_yearly_amp_range),\n",
    "        noise_mean = random.randint(*noise_mean_range),\n",
    "        noise_std = random.randint(*noise_std_range),\n",
    "        noise_corr = random.uniform(*noise_corr_range)\n",
    "        ) \n",
    "    \n",
    "    df_single_partition = create_dataframe(time_points, ts_values)\n",
    "\n",
    "\n",
    "    # Add the partition columns to the DataFrame\n",
    "    if partition_columns:\n",
    "        for i in range(len(partition_columns)):\n",
    "            df_single_partition[partition_columns[i]] = combo[i]\n",
    "\n",
    "    # Add exogenous features that are correlated with the target\n",
    "    if exogenous_feature_columns:\n",
    "        target_stddev_hourly = df_single_partition['TARGET'].std()\n",
    "\n",
    "        for i in range(len(exogenous_feature_columns)):\n",
    "            df_single_partition[exogenous_feature_columns[i]] = (\n",
    "                df_single_partition['TARGET'] + \n",
    "                np.random.normal(loc=0, scale=stddev_scale_factors[i] * target_stddev_hourly, size=len(df_single_partition)) \n",
    "                )\n",
    "\n",
    "    # Append the partition's DataFrame to the list\n",
    "    df_list.append(df_single_partition)\n",
    "\n",
    "# # Combine all partition DataFrames into a single DataFrame\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# NOTE: 50,000 partitions took 4.5 minutes to generate and combine into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at df\n",
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display correlation coefficients between exogenous features and target\n",
    "print(\"CORRELATION COEFFICIENT WITH TARGET \\n\")\n",
    "for col in exogenous_feature_columns:\n",
    "    print(f\"{col}: {round(df_all['TARGET'].corr(df_all[col]), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Snowpark DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = session.create_dataframe(df_all)\n",
    "\n",
    "# NOTE: The pandas df with 50,000 partitions took 18 minutes to turn into a Snowpark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.describe().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write tables to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.write.save_as_table(f\"{granularity.upper()}_TS_{number_of_partitions}_PARTITIONS_STARTING_{start_year}\", mode=\"overwrite\")\n",
    "print(f\"Created Table: {granularity.upper()}_TS_{number_of_partitions}_PARTITIONS_STARTING_{start_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize result of above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORE: Look at plots for different partitions\n",
    "for combo in all_combos[:5]:\n",
    "    source_df = df_all\n",
    "    x = 1095\n",
    "\n",
    "    #df = source_df.loc[(source_df['STORE_ID']==combo[0]) & (source_df['PRODUCT_ID']==combo[1])].copy()\n",
    "    df = source_df.loc[(source_df['STORE_ID']==combo[0])].copy()\n",
    "    df.set_index('ORDER_TIMESTAMP', inplace=True)\n",
    "\n",
    "    recent_x_months = df.loc[df.index >= pd.Timestamp.now() - pd.DateOffset(days=x)]\n",
    "\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.plot(recent_x_months.index, recent_x_months['TARGET'], marker='o', linestyle='-')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Target')\n",
    "    plt.title(f'Time Series Data - Last {x} Days')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a calendar table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_table(start='2018-01-01', end='2025-12-31'):\n",
    "    df = pd.DataFrame({\"CALENDAR_DATE\": pd.date_range(start, end)})\n",
    "    df[\"CALENDAR_WEEK_DAY_NBR\"] = df.CALENDAR_DATE.dt.dayofweek\n",
    "    df[\"CALENDAR_MTH_DAY_NBR\"] = df.CALENDAR_DATE.dt.day\n",
    "    df[\"CALENDAR_MTH\"] = df.CALENDAR_DATE.dt.month\n",
    "    df[\"CALENDAR_YEAR\"] = df.CALENDAR_DATE.dt.year\n",
    "    return df\n",
    "\n",
    "calendar_df = create_date_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = USFederalHolidayCalendar()\n",
    "holiday_df = (\n",
    "    pd.DataFrame(\n",
    "        calendar.holidays(start='2018-01-01', end='2025-12-31', return_name=True)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"date\", 0: \"holiday_name\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df['date'] = holiday_df['date'].dt.date\n",
    "calendar_df['CALENDAR_DATE'] = calendar_df['CALENDAR_DATE'].dt.date\n",
    "calendar_final = calendar_df.merge(holiday_df, left_on='CALENDAR_DATE', right_on='date', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_final = calendar_final.rename(columns={\"holiday_name\":\"HOLIDAY_NAME\"})\n",
    "calendar_final_snow_df = session.create_dataframe(calendar_final).select('CALENDAR_DATE','CALENDAR_WEEK_DAY_NBR','CALENDAR_MTH_DAY_NBR','CALENDAR_MTH','CALENDAR_YEAR','HOLIDAY_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_final_snow_df = calendar_final_snow_df.\\\n",
    "select(col('CALENDAR_DATE'),\\\n",
    "       col('CALENDAR_WEEK_DAY_NBR').cast(StringType()).alias('WEEK_DAY_NBR'),\\\n",
    "       col('CALENDAR_MTH_DAY_NBR').cast(StringType()).alias('MTH_DAY_NBR'),\\\n",
    "       col('CALENDAR_MTH').cast(StringType()).alias('CALENDAR_MTH'),\\\n",
    "       col('CALENDAR_YEAR').cast(StringType()).alias('CALENDAR_YEAR'),\\\n",
    "       col('HOLIDAY_NAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------\n",
      "|\"CALENDAR_DATE\"  |\"WEEK_DAY_NBR\"  |\"MTH_DAY_NBR\"  |\"CALENDAR_MTH\"  |\"CALENDAR_YEAR\"  |\"HOLIDAY_NAME\"  |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "|2018-01-01       |0               |1              |1               |2018             |New Year's Day  |\n",
      "|2018-01-02       |1               |2              |1               |2018             |NULL            |\n",
      "|2018-01-03       |2               |3              |1               |2018             |NULL            |\n",
      "|2018-01-04       |3               |4              |1               |2018             |NULL            |\n",
      "|2018-01-05       |4               |5              |1               |2018             |NULL            |\n",
      "|2018-01-06       |5               |6              |1               |2018             |NULL            |\n",
      "|2018-01-07       |6               |7              |1               |2018             |NULL            |\n",
      "|2018-01-08       |0               |8              |1               |2018             |NULL            |\n",
      "|2018-01-09       |1               |9              |1               |2018             |NULL            |\n",
      "|2018-01-10       |2               |10             |1               |2018             |NULL            |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calendar_final_snow_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_final_snow_df.write.saveAsTable('cromano.mockseries.CALENDAR_INFO_2018', mode='overwrite', create_temp_table=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
